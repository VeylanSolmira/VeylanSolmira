### Hi, I'm Veylan ðŸ‘‹

I'm an independent researcher and engineer transitioning into full-time work in AI safety and alignment. I bring an interdisciplinary background in computational biology, computer science, philosophy, and systems engineeringâ€”with professional experience across data engineering, software development, and site reliability at scale.

---

ðŸ”­ **Currently working on**  
- Prototype alignment environments for testing deceptive or misgeneralizing agents  
- GaiaLogos: a research testbed for ecologically-bounded agent development  
- Compression-based evaluation frameworks inspired by historiographical modeling

ðŸŒ± **Currently learning**  
- Red/blue team evaluation techniques (`Inspect`, alignment-faking games)  
- Recursive, epistemically-updating reward shaping  
- Multi-objective reinforcement learning and scalable oversight design  

ðŸ‘¯ **Looking to collaborate on**  
- Interpretability tools, deception detection, or oversight systems  
- Evaluation frameworks that combine philosophical grounding and engineering fluency  
- Alignment projects that treat AI as embedded in an ecological and civilizational context

ðŸ¤” **Open to help with**  
- Understanding large-scale interpretability workflows (SAEs, tracing tools)  
- Publishing or productionizing early-stage alignment tools  
- Connecting with mentors or institutions working on practical safety architectures

ðŸ’¬ **Ask me about**  
- Deceptive alignment, narrative compression, AI persuasion risks  
- How ecology, ethics, and epistemology can be computationally encoded  
- Philosophical perspectives on reward shaping and value alignment

ðŸ“« **Reach me at**  
[Twitter](https://twitter.com/VeylanSolmira) â€¢ [LinkedIn](https://www.linkedin.com/in/veylansolmira) â€¢ [Email](mailto:veylan.solmira@gmail.com)

ðŸ˜„ **Pronouns:** he/they  
âš¡ **Fun fact:** I'm working on reading through the [top-100 books of all time](https://thegreatestbooks.org/). I'm currently on 47/100.
